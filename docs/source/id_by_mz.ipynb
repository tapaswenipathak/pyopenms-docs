{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification by Accurate Mass\n",
    "\n",
    "Example workflow for the processing of a set of mzML files (defined in\n",
    "the `files` variable) including centroiding, feature detection, feature\n",
    "linking and accurate mass search. The resulting data gets processed in a\n",
    "pandas data frame with feature filtering (missing values, quality) and\n",
    "imputation of remaining missing values. Compounds detected during\n",
    "accurate mass search will be annoted in the resulting dataframe.\n",
    "\n",
    "## Imports and mzML file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyopenms import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set path to your mzML files, or leave like this to use the example data\n",
    "files = os.path.join(os.getcwd(), \"IdByMz_Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Example Data\n",
    "\n",
    "Execute this cell only for the example workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(os.getcwd(), 'IdByMz_Example')):\n",
    "    os.mkdir(os.path.join(os.getcwd(), 'IdByMz_Example'))\n",
    "\n",
    "base = 'https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Tutorials/Data/latest/Example_Data/Metabolomics/'\n",
    "urls = ['datasets/2012_02_03_PStd_050_1.mzML',\n",
    "        'datasets/2012_02_03_PStd_050_2.mzML',\n",
    "        'datasets/2012_02_03_PStd_050_3.mzML',\n",
    "        'databases/PositiveAdducts.tsv',\n",
    "        'databases/NegativeAdducts.tsv',\n",
    "        'databases/HMDBMappingFile.tsv',\n",
    "        'databases/HMDB2StructMapping.tsv']\n",
    "\n",
    "for url in urls:\n",
    "    request = requests.get(base + url, allow_redirects=True)\n",
    "    open(os.path.join(files, os.path.basename(url)), 'wb').write(request.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroiding\n",
    "\n",
    "If files are already centroided this step can bet omitted.\n",
    "\n",
    "in: path to MS data (files)\n",
    "\n",
    "out: path to centroided mzML files in a subfolder 'centroid' (files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(files, \"centroid\")):\n",
    "    shutil.rmtree(os.path.join(files, \"centroid\"))\n",
    "os.mkdir(os.path.join(files, \"centroid\"))\n",
    "\n",
    "for file in os.listdir(files):\n",
    "\n",
    "    if file.endswith(\".mzML\"):\n",
    "        exp_raw = MSExperiment()\n",
    "        MzMLFile().load(os.path.join(files, file), exp_raw)\n",
    "        exp_centroid = MSExperiment()\n",
    "\n",
    "        PeakPickerHiRes().pickExperiment(exp_raw, exp_centroid, True)\n",
    "\n",
    "        MzMLFile().store(os.path.join(files, \"centroid\", file), exp_centroid)\n",
    "        del exp_raw\n",
    "\n",
    "files = os.path.join(files, \"centroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Detection\n",
    "\n",
    "in: path to centroided mzML files (files)\n",
    "\n",
    "out: list with FeatureMaps (feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = []\n",
    "\n",
    "for file in os.listdir(files):\n",
    "\n",
    "    if file.endswith(\".mzML\"):\n",
    "        exp = MSExperiment()\n",
    "        MzMLFile().load(os.path.join(files, file), exp)\n",
    "\n",
    "        exp.sortSpectra(True)\n",
    "\n",
    "        mass_traces = []\n",
    "        mtd = MassTraceDetection()\n",
    "        mtd_params = mtd.getDefaults()\n",
    "        mtd_params.setValue(\n",
    "            \"mass_error_ppm\", 5.0\n",
    "        )  # set according to your instrument mass error\n",
    "        mtd_params.setValue(\n",
    "            \"noise_threshold_int\", 1000.0\n",
    "        )  # adjust to noise level in your data\n",
    "        mtd.setParameters(mtd_params)\n",
    "        mtd.run(exp, mass_traces, 0)\n",
    "\n",
    "        mass_traces_split = []\n",
    "        mass_traces_final = []\n",
    "        epd = ElutionPeakDetection()\n",
    "        epd_params = epd.getDefaults()\n",
    "        epd_params.setValue(\"width_filtering\", \"fixed\")\n",
    "        epd.setParameters(epd_params)\n",
    "        epd.detectPeaks(mass_traces, mass_traces_split)\n",
    "\n",
    "        if epd.getParameters().getValue(\"width_filtering\") == \"auto\":\n",
    "            epd.filterByPeakWidth(mass_traces_split, mass_traces_final)\n",
    "        else:\n",
    "            mass_traces_final = mass_traces_split\n",
    "\n",
    "        feature_map = FeatureMap()\n",
    "        feat_chrom = []\n",
    "        ffm = FeatureFindingMetabo()\n",
    "        ffm_params = ffm.getDefaults()\n",
    "        ffm_params.setValue(\"isotope_filtering_model\", \"none\")\n",
    "        ffm_params.setValue(\n",
    "            \"remove_single_traces\", \"true\"\n",
    "        )  # set false to keep features with only one mass trace\n",
    "        ffm_params.setValue(\"mz_scoring_by_elements\", \"false\")\n",
    "        ffm_params.setValue(\"report_convex_hulls\", \"true\")\n",
    "        ffm.setParameters(ffm_params)\n",
    "        ffm.run(mass_traces_final, feature_map, feat_chrom)\n",
    "\n",
    "        feature_map.setUniqueIds()\n",
    "        feature_map.setPrimaryMSRunPath([file[:-5].encode()])\n",
    "\n",
    "        feature_maps.append(feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Map Retention Time Alignment\n",
    "\n",
    "in: unaligned feature maps (feature_maps)\n",
    "\n",
    "out: feature maps aligned on the first feature map in the list\n",
    "(feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get in index of feature map with highest number of features in feature map list\n",
    "ref_index = [i[0] for i in sorted(\n",
    "    enumerate([fm.size() for fm in feature_maps]), key=lambda x: x[1])][-1]\n",
    "\n",
    "aligner = MapAlignmentAlgorithmPoseClustering()\n",
    "\n",
    "aligner.setReference(feature_maps[ref_index])\n",
    "\n",
    "for feature_map in feature_maps[:ref_index] + feature_maps[ref_index + 1:]:\n",
    "    trafo = TransformationDescription()\n",
    "    aligner.align(feature_map, trafo)\n",
    "    transformer = MapAlignmentTransformer()\n",
    "    transformer.transformRetentionTimes(\n",
    "        feature_map, trafo, True\n",
    "    )  # store original RT as meta value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of RTs before and after alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmaps = (\n",
    "    [feature_maps[ref_index]] + feature_maps[:ref_index] +\n",
    "    feature_maps[ref_index + 1:]\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"consensus map before alignment\")\n",
    "ax.set_ylabel(\"m/z\")\n",
    "ax.set_xlabel(\"RT\")\n",
    "\n",
    "# use alpha value to display feature intensity\n",
    "ax.scatter(\n",
    "    [f.getRT() for f in fmaps[0]],\n",
    "    [f.getMZ() for f in fmaps[0]],\n",
    "    alpha=np.asarray([f.getIntensity() for f in fmaps[0]])\n",
    "    / max([f.getIntensity() for f in fmaps[0]]),\n",
    ")\n",
    "\n",
    "for fm in fmaps[1:]:\n",
    "    ax.scatter(\n",
    "        [f.getMetaValue(\"original_RT\") for f in fm],\n",
    "        [f.getMZ() for f in fm],\n",
    "        alpha=np.asarray([f.getIntensity() for f in fm])\n",
    "        / max([f.getIntensity() for f in fm]),\n",
    "    )\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"consensus map after alignment\")\n",
    "ax.set_xlabel(\"RT\")\n",
    "\n",
    "for fm in fmaps:\n",
    "    ax.scatter(\n",
    "        [f.getRT() for f in fm],\n",
    "        [f.getMZ() for f in fm],\n",
    "        alpha=np.asarray([f.getIntensity() for f in fm])\n",
    "        / max([f.getIntensity() for f in fm]),\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.legend(\n",
    "    [fmap.getMetaValue(\"spectra_data\")[0].decode() for fmap in fmaps],\n",
    "    loc=\"lower center\",\n",
    ")\n",
    "# in some cases get file name elsewhere, e.g. fmap.getDataProcessing()[0].getMetaValue('parameter: out')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Linking\n",
    "\n",
    "in: list with FeatureMaps (feature_maps)\n",
    "\n",
    "out: ConsensusMap (consensus_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_grouper = FeatureGroupingAlgorithmQT()\n",
    "\n",
    "consensus_map = ConsensusMap()\n",
    "file_descriptions = consensus_map.getColumnHeaders()\n",
    "\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    file_description = file_descriptions.get(i, ColumnHeader())\n",
    "    file_description.filename = feature_map.getMetaValue(\"spectra_data\")[0].decode()\n",
    "    file_description.size = feature_map.size()\n",
    "    file_description.unique_id = feature_map.getUniqueId()\n",
    "    file_descriptions[i] = file_description\n",
    "\n",
    "consensus_map.setColumnHeaders(file_descriptions)\n",
    "feature_grouper.group(feature_maps, consensus_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConsensusMap to pandas DataFrame\n",
    "\n",
    "in: ConsensusMap (consensus_map)\n",
    "\n",
    "out: DataFrame with RT, mz and quality from ConsensusMap (cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = consensus_map.get_intensity_df()\n",
    "\n",
    "meta_data = consensus_map.get_metadata_df()[[\"RT\", \"mz\", \"quality\"]]\n",
    "\n",
    "cm_df = pd.concat([meta_data, intensities], axis=1)\n",
    "cm_df.reset_index(drop=True, inplace=True)\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accurate Mass Search\n",
    "\n",
    "in: ConsensusMap (consensus_map)\n",
    "\n",
    "out: DataFrame with AccurateMassSearch results (ams_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if files.endswith('centroid'):\n",
    "    files = os.path.join(files, '..')\n",
    "\n",
    "ams = AccurateMassSearchEngine()\n",
    "\n",
    "ams_params = ams.getParameters()\n",
    "ams_params.setValue(\"ionization_mode\", \"negative\")\n",
    "ams_params.setValue(\"positive_adducts\", os.path.join(\n",
    "    files, \"PositiveAdducts.tsv\"))\n",
    "ams_params.setValue(\"negative_adducts\", os.path.join(\n",
    "    files, \"NegativeAdducts.tsv\"))\n",
    "ams_params.setValue(\"db:mapping\", [os.path.join(files, \"HMDBMappingFile.tsv\")])\n",
    "ams_params.setValue(\n",
    "    \"db:struct\", [os.path.join(files, \"HMDB2StructMapping.tsv\")])\n",
    "ams.setParameters(ams_params)\n",
    "\n",
    "mztab = MzTab()\n",
    "\n",
    "ams.init()\n",
    "\n",
    "ams.run(consensus_map, mztab)\n",
    "\n",
    "MzTabFile().store(os.path.join(files, \"ids.tsv\"), mztab)\n",
    "\n",
    "df = pd.read_csv(os.path.join(files, \"ids.tsv\"), header=None, sep=\"\\n\")\n",
    "df = df[0].str.split(\"\\t\", expand=True)\n",
    "\n",
    "ams_df = df.loc[df[0] == \"SML\"]\n",
    "ams_df.columns = df.loc[df[0] == \"SMH\"].iloc[0]\n",
    "\n",
    "os.remove(os.path.join(files, \"ids.tsv\"))\n",
    "\n",
    "ams_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering and Imputation\n",
    "\n",
    "in: unfiltered ConsensusMap DataFrame (cm_df)\n",
    "\n",
    "out: features below minimum quality and with too many missing values\n",
    "removed, remaining missing values imputated with KNN algorithm (cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_missing_values = 1\n",
    "min_feature_quality = 0.8\n",
    "n_nearest_neighbours = 2\n",
    "\n",
    "# drop features that have more then the allowed number of missing values or are below minimum feature quality\n",
    "to_drop = []\n",
    "\n",
    "for i, row in cm_df.iterrows():\n",
    "    if (\n",
    "        row.isna().sum() > allowed_missing_values\n",
    "        or row[\"quality\"] < min_feature_quality\n",
    "    ):\n",
    "        to_drop.append(i)\n",
    "\n",
    "cm_df.drop(index=cm_df.index[to_drop], inplace=True)\n",
    "\n",
    "# Data imputation with KNN\n",
    "imputer = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", KNNImputer(n_neighbors=2)),\n",
    "        (\n",
    "            \"pandarizer\",\n",
    "            FunctionTransformer(lambda x: pd.DataFrame(\n",
    "                x, columns=cm_df.columns)),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cm_df = imputer.fit_transform(cm_df)\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate features with identified compounds\n",
    "\n",
    "in: ConsensusMap DataFrame without identifications (cm_df) and\n",
    "AccurateMassSearch DataFrame (ams_df)\n",
    "\n",
    "out: ConsensusMap DataFrame with new identifications column (id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = cm_df\n",
    "\n",
    "id_df[\"identifications\"] = pd.Series(\n",
    "    [\"\" for x in range(len(id_df.index))])\n",
    "\n",
    "for rt, mz, description in zip(\n",
    "    ams_df[\"retention_time\"], ams_df[\"exp_mass_to_charge\"], ams_df[\"description\"]\n",
    "):\n",
    "    indices = id_df.index[\n",
    "        np.isclose(id_df[\"mz\"], float(mz), atol=1e-05)\n",
    "        & np.isclose(id_df[\"RT\"], float(rt), atol=1e-05)\n",
    "    ].tolist()\n",
    "    for index in indices:\n",
    "        if description != \"null\":\n",
    "            id_df.loc[index, \"identifications\"] += description + \";\"\n",
    "id_df[\"identifications\"] = [\n",
    "    item[:-1] if \";\" in item else \"\" for item in id_df[\"identifications\"]\n",
    "]\n",
    "id_df.to_csv(os.path.join(files, \"result.tsv\"), sep=\"\\t\", index=False)\n",
    "id_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize consensus features with identifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(id_df, x=\"RT\", y=\"mz\", hover_name=\"identifications\")\n",
    "fig.update_layout(title=\"Consensus features with identifications (hover)\")\n",
    "fig.show()"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
